{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods\n",
    "\n",
    "This notebook follows ISLR chapter 5 which is about using resampling methods to verify our analytics approach and includes the following topics:\n",
    "* Validation Set\n",
    "* Leave-One-Out Cross Validation (LOOCV)\n",
    "* K-fold Cross Validation\n",
    "* Bias-Variance Tradeoff for K-fold Cross Validation\n",
    "* Cross-Validation in Classification Problems\n",
    "* Bootstrap\n",
    "* Applied Cross Validation on ISLR Auto Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Repeatedly sampling data from a training data set and refitting our model to gauge if the fitted models differ. Training models this way gives us more insight into a model's performance. The two most commonly used resampling methods are:\n",
    "\n",
    "1. Cross Validation (CV): Used to evaluate the test error of the model or to select the level of flexibility of a model.\n",
    "    * Evaluating model performance is known as ***model assessment***\n",
    "    * Selecting the proper level of flexibility is known as ***model selection***\n",
    "2. Bootstrap: Measure the accuracy of a parameter estimate of a learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Error and Test Error\n",
    "\n",
    "***Test Error***: The average error obtained from using a model to make a prediction on new data.\n",
    "\n",
    "***Training Error:*** The average error obtained from using a model to make predictions against the training set of data.\n",
    "\n",
    "We always evaluate model performance using the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set\n",
    "\n",
    "One way to evaluate Training/Test error is to split the data into a training and test (or hold-out) dataset. Our modeling approach follows these steps:\n",
    "\n",
    "1. Divide data into training and test set***\n",
    "2. Fit model on training dataset\n",
    "3. Use fitted model to predict the responses for the data points in the test set\n",
    "4. Evaluate model performance using a measure of error like MSE\n",
    "\n",
    "***If comparing multiple models it is best to divide data into a training/test/validation set, comparing different models against the test dataset, and evaluating the final model chosen against the validation set\n",
    "\n",
    "<img src=\"Training_Test.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying validation set approach to ISLR `Default` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "data(Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>$names</strong> = <ol class=list-inline>\n",
       "\t<li>'train'</li>\n",
       "\t<li>'test'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\textbf{\\$names} = \\begin{enumerate*}\n",
       "\\item 'train'\n",
       "\\item 'test'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "**$names** = 1. 'train'\n",
       "2. 'test'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$names\n",
       "[1] \"train\" \"test\" \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec <- c(train = .8, test = .2)\n",
    "\n",
    "g <- sample(cut(\n",
    "  seq(nrow(Default)), \n",
    "  nrow(Default)*cumsum(c(0,spec)),\n",
    "  labels = names(spec)\n",
    "))\n",
    "\n",
    "split_data_set <- split(Default, g)\n",
    "attributes(split_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>train</dt>\n",
       "\t\t<dd>0.8</dd>\n",
       "\t<dt>test</dt>\n",
       "\t\t<dd>0.2</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[train] 0.8\n",
       "\\item[test] 0.2\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "train\n",
       ":   0.8test\n",
       ":   0.2\n",
       "\n"
      ],
      "text/plain": [
       "train  test \n",
       "  0.8   0.2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sapply(split_data_set, nrow)/nrow(Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the dataset into a training and a test set we can proceed to train a machine learning model on the training set and evaluate model performance on the test set.\n",
    "\n",
    "Alternatively, we could split our data into training, test, and validation sets if we are evaluating more then one model. We train our prospective models on the training dataset, compare model performance on the test dataset, and finally evaluate the chosen model's \"true\" predictive capability on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>$names</strong> = <ol class=list-inline>\n",
       "\t<li>'train'</li>\n",
       "\t<li>'test'</li>\n",
       "\t<li>'validation'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\textbf{\\$names} = \\begin{enumerate*}\n",
       "\\item 'train'\n",
       "\\item 'test'\n",
       "\\item 'validation'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "**$names** = 1. 'train'\n",
       "2. 'test'\n",
       "3. 'validation'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$names\n",
       "[1] \"train\"      \"test\"       \"validation\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec <- c(train = .7, test = .2, validation=.1)\n",
    "\n",
    "g <- sample(cut(\n",
    "  seq(nrow(Default)), \n",
    "  nrow(Default)*cumsum(c(0,spec)),\n",
    "  labels = names(spec)\n",
    "))\n",
    "\n",
    "split_data_set <- split(Default, g)\n",
    "attributes(split_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>train</dt>\n",
       "\t\t<dd>0.7</dd>\n",
       "\t<dt>test</dt>\n",
       "\t\t<dd>0.2</dd>\n",
       "\t<dt>validation</dt>\n",
       "\t\t<dd>0.1</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[train] 0.7\n",
       "\\item[test] 0.2\n",
       "\\item[validation] 0.1\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "train\n",
       ":   0.7test\n",
       ":   0.2validation\n",
       ":   0.1\n",
       "\n"
      ],
      "text/plain": [
       "     train       test validation \n",
       "       0.7        0.2        0.1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sapply(split_data_set, nrow)/nrow(Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of Validation Set Approach\n",
    "\n",
    "While easy to understand and implement, the validation set appproach suffers from a few issues:\n",
    "\n",
    "* By nature, splitting the data into two datasets is highly variable in the sense that our validation test error estimate is contingent upon which observations were included in the training and validation sets\n",
    "* Since one set of data is used to fit the model, the test set error may tend to over-estimate the true test error of the model because it is biased based on which observations were being tested on\n",
    "\n",
    "***Cross Validation*** addresses both of these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation (LOOCV)\n",
    "\n",
    "LOOCV also uses a training and a test dataset, but is structured as follows:\n",
    "* Only one observation $(x_{1} \\, , \\, y_{1})$ is used for the test set\n",
    "* All other $(n-1)$ observations $\\left \\{ (x_{2} \\, , \\, y_{2}) \\, , \\, \\ldots \\, , \\, (x_{n} \\, , \\, y_{n}) \\right \\}$ are used to train the model\n",
    "* A prediction $\\hat{y_{1}}$ is made on the test datapoint $x_{1}$\n",
    "* Calculate the error term, for example $MSE_{1}=(y_{1} - \\hat{y_{1}})^{2}$\n",
    "    * This error term is an approximately unbiased estimate for the test error\n",
    "    * However, this test error is highly variable as it is made up of only one data point!\n",
    "\n",
    "To minimize the variance associated with this method, we repeat the whole procedure using all $n$ data points to come up with $MSE_{1} \\, , \\, \\ldots \\, , \\, MSE_{n}$\n",
    "\n",
    "The LOOCV estimate for the test MSE is the average of these $n$ MSEs:\n",
    "\n",
    "$$\n",
    "CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}MSE_{i}\n",
    "$$\n",
    "\n",
    "<img src=\"LOOCV.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying LOOCV approach to ISLR `Default` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: boot\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "“there is no package called ‘boot’”Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n"
     ]
    }
   ],
   "source": [
    "if (!require(boot)) install.packages(\"boot\")\n",
    "library(boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(cv.glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>default</th><th scope=col>student</th><th scope=col>balance</th><th scope=col>income</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 729.5265</td><td>44361.63 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 817.1804</td><td>12106.13 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1073.5492</td><td>31767.14 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " default & student & balance & income\\\\\n",
       "\\hline\n",
       "\t No        & No        &  729.5265 & 44361.63 \\\\\n",
       "\t No        & Yes       &  817.1804 & 12106.13 \\\\\n",
       "\t No        & No        & 1073.5492 & 31767.14 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "default | student | balance | income | \n",
       "|---|---|---|\n",
       "| No        | No        |  729.5265 | 44361.63  | \n",
       "| No        | Yes       |  817.1804 | 12106.13  | \n",
       "| No        | No        | 1073.5492 | 31767.14  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  default student balance   income  \n",
       "1 No      No       729.5265 44361.63\n",
       "2 No      Yes      817.1804 12106.13\n",
       "3 No      No      1073.5492 31767.14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(Default,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Default$dft <- ifelse(Default$default == \"Yes\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>default</th><th scope=col>student</th><th scope=col>balance</th><th scope=col>income</th><th scope=col>dft</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 729.5265</td><td>44361.63 </td><td>0        </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 817.1804</td><td>12106.13 </td><td>0        </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1073.5492</td><td>31767.14 </td><td>0        </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " default & student & balance & income & dft\\\\\n",
       "\\hline\n",
       "\t No        & No        &  729.5265 & 44361.63  & 0        \\\\\n",
       "\t No        & Yes       &  817.1804 & 12106.13  & 0        \\\\\n",
       "\t No        & No        & 1073.5492 & 31767.14  & 0        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "default | student | balance | income | dft | \n",
       "|---|---|---|\n",
       "| No        | No        |  729.5265 | 44361.63  | 0         | \n",
       "| No        | Yes       |  817.1804 | 12106.13  | 0         | \n",
       "| No        | No        | 1073.5492 | 31767.14  | 0         | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  default student balance   income   dft\n",
       "1 No      No       729.5265 44361.63 0  \n",
       "2 No      Yes      817.1804 12106.13 0  \n",
       "3 No      No      1073.5492 31767.14 0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(Default,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://gerardnico.com/lang/r/cross_validation#leave-one-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model with a quantitative response variable to demonstrate LOOCV. Later we'll apply the exact same logic to a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glm.fit <- glm(balance~dft+income, data=Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LOOCV\n",
    "loocv <- cv.glm(Default,glm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the `delta` attribute of the loocv object returned to get the measure of $CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}MSE_{i}$\n",
    "* First term is our MSE\n",
    "* Second term is the bias corrected MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loocv$delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Using LOOCV\n",
    "\n",
    "* Pros:\n",
    "    * LOOCV has far less bias then the training/test approach since it uses $(n-1)$ observations to train the model\n",
    "    * After repeating the process for all $n$ data points we end up with a measure of model performance that has used all data points to train/test the model\n",
    "    * LOOCV always gives the same results, unlike the training/test approach\n",
    "* Cons:\n",
    "    * Computationally expensive since we must do $n$ model runs\n",
    "    * Often infeasible for very large datasets or for very computationally intense model runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold CV\n",
    "\n",
    "Another variant of cross validation is ***K-fold CV***:\n",
    "* Divide the data into $k$ groups or folds of about the same size\n",
    "* Training the model on $(k-1)$ folds, testing on the hold out fold\n",
    "* Measuring the model error using a measure like MSE\n",
    "* Repeating the procedure $k$ times yielding $k$ estimates of the test error $MSE_{1} \\, , \\, \\ldots \\, , \\, MSE_{k}$\n",
    "* k-fold CV estimate for the test MSE is the average of these $k$ MSEs:\n",
    "\n",
    "$$\n",
    "CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k}MSE_{i}\n",
    "$$\n",
    "\n",
    "***LOOCV is a special case of K-fold CV where $k=n$***\n",
    "\n",
    "<img src=\"k-fold.PNG\">\n",
    "\n",
    "K-fold CV is a better choice then LOOCV because we only need to perform $k$ model runs compared to $n$ model runs with LOOCV. This is advantageous with large datasets or complex model runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying k-fold CV approach to ISLR `Default` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(cv.glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_fold <- cv.glm(Default, glm.fit, K = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the `delta` attribute of the k_fold object returned to get the measure of $CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k}MSE_{i}$\n",
    "* First term is our MSE\n",
    "* Second term is the bias corrected MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>200491.722513726</li>\n",
       "\t<li>200460.847452751</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 200491.722513726\n",
       "\\item 200460.847452751\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 200491.722513726\n",
       "2. 200460.847452751\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 200491.7 200460.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_fold$delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Using k-fold CV\n",
    "\n",
    "* Pros:\n",
    "    * Model is only fit $k$ time compared to $n$ times with LOOCV\n",
    "    * After repeating the process for all $k$ folds we end up with a measure of model performance that has used all data points to train/test the model\n",
    "* Cons:\n",
    "    * Can still be computationally expensive since we must do $k$ model runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff with k-fold CV\n",
    "\n",
    "* Since model is only fit $k$ times compared to $n$ times with LOOCV (where $k<n$), k-fold CV is computationally less expensive\n",
    "* Believe it or not ***k-fold CV gives more accurate test error rate estimates than LOOCV***\n",
    "    * This is due to the bias-variance tradeoff\n",
    "    * LOOCV yields approximately unbiased estimates of the test error since each training set has (n-1) observations which is basically the whole dataset\n",
    "    * The fact above makes LOOCV basically equivalent to the test/training approach when it comes to underestimating the true test error rate\n",
    "    \n",
    "Doing k-fold CV with $k=5$ or $k=10$ will lead to some level of bias since each test set contains $\\frac{n}{k}$ observations and each training set contains $n-\\frac{n}{k}=n\\left(1-\\frac{1}{k}\\right)=n\\frac{\\left(k-1\\right)}{k}$ observations.\n",
    "* Training set size in k-fold CV is smaller than in LOOCV, but alot bigger than the test/training approach\n",
    "\n",
    "From a test error bias reduction standpoint, ***LOOCV is preferred to k-fold CV***\n",
    "\n",
    "However, ***LOOCV has higher test error rate variance than k-fold CV with $k<n$***\n",
    "* Outputs from LOOCV are highly correlated with one another\n",
    "* Due to the fact that the mean of many highly correlated quantities has higher variance than the mean of many quantities that are not highly correlated (k-fold CV)\n",
    "\n",
    "LOOCV is in effect averaging the outputs of $n$ fitted models, each of which is trained on an almost identical set of data\n",
    "\n",
    "In contrast, when we perform k-fold CV with $k<n$, we are averaging the outputs of $k$ fitted models that are somewhat less correlated with each other since the overlap between training sets in each model is smaller\n",
    "* Use $k=5$ or $k=10$ as a sweet spot tradeoff between bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation in Classification Problems\n",
    "\n",
    "We can use the exact same resampling methods discussed above in classification problems.The only thing that changes is the error term (by default we only look at the misclassification rate, but could just as easily extend this to other classification metrics like F1 score):\n",
    "\n",
    "$$\n",
    "CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}Err_{i}\n",
    "$$\n",
    "\n",
    "where $ERR_{i} = I\\,(y_{i} \\neq \\hat{y_{i}})$ i.e., $ERR_{i} = 1\\: if\\: (y_{i} \\neq \\hat{y_{i}})$ and 0 otherwise.\n",
    "\n",
    "This formulation of the error term holds for the training/test approach, LOOCV, and k-fold CV.\n",
    "\n",
    "<img src=\"CV Classification.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charts above further demonstrate the need for cross-validation. It should be obvious that as the model gets more and more complex, the training data is fit better and better. This perceived high confidence in model fit to the training data can be very misleading as evidenced by the \"U\" shape of the test set in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "Bootstrap is a widely applicable and powerful tool to ***quantify the uncertainty*** associated with a given estimator or machine learning model.\n",
    "* For example, bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit\n",
    "* Bootstrap is powerful because we can find measures of variability that are not automatically output by statistical software\n",
    "* See ISLR Page 195-197 for more details on lab below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "## ISLR Bootstrap Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
